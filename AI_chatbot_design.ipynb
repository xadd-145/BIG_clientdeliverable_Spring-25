{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipdBqG8Zi2EL"
      },
      "source": [
        "### AI Chatbot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-c0IkQ-Qi0tv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3cb58db-a72d-4675-b444-47b632a091bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May  5 19:19:18 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "N4lSaS_-i0wr",
        "outputId": "80c9002b-7eee-4451-e8e6-3f096532a6cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.76.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.56)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.39)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain) (3.0.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install langchain openai faiss-cpu pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Hig3Jdji0zS",
        "outputId": "3123ce33-6849-468d-fbea-fb77b18cb89d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded document with 318250 words.\n"
          ]
        }
      ],
      "source": [
        "# Load the Extracted Raw Text\n",
        "import os\n",
        "\n",
        "# Load the raw text file extracted using AWS Textract\n",
        "with open(\"COMBINE_3.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    raw_text = file.read()\n",
        "\n",
        "# Ensure the text is loaded properly\n",
        "print(\"Loaded document with\", len(raw_text.split()), \"words.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sBMW7OVi015",
        "outputId": "28312b7e-e854-475c-e90d-ae6e59d329dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks created: 1152\n"
          ]
        }
      ],
      "source": [
        "#Chunk the Data for Processing\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# Define a text splitter to break long documents into smaller, retrievable chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=2500,  # Each chunk has 2500 characters\n",
        "    chunk_overlap=300  # Overlapping text to maintain context\n",
        ")\n",
        "\n",
        "# Split raw text into chunks\n",
        "docs = text_splitter.split_text(raw_text)\n",
        "\n",
        "print(f\"Total chunks created: {len(docs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9VvtukLi04Y"
      },
      "outputs": [],
      "source": [
        "#Create Embeddings and Store in FAISS\n",
        "! pip install langchain_community\n",
        "! pip install tiktoken\n",
        "from langchain_community.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Load OpenAI embeddings (or use any other embedding model)\n",
        "embedding_model = OpenAIEmbeddings(openai_api_key= \"OpenAI_API_Key\")\n",
        "\n",
        "# Store document chunks in FAISS for fast retrieval\n",
        "vectorstore = FAISS.from_texts(docs, embedding_model)\n",
        "\n",
        "# Save the FAISS index for future use\n",
        "vectorstore.save_local(\"faiss_cba_index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAyXNrnSi06y"
      },
      "outputs": [],
      "source": [
        "#Build the Chatbot Using LangChain\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Load FAISS index\n",
        "vectorstore = FAISS.load_local(\"faiss_cba_index\", embedding_model, allow_dangerous_deserialization=True)\n",
        "\n",
        "# Use GPT-4 model for high accuracy responses\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", openai_api_key=\"OpenAI_API_Key\")\n",
        "\n",
        "# Create a retrieval-based QA system\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),  # Retrieve top 3 relevant chunks\n",
        "    chain_type=\"stuff\"\n",
        ")\n",
        "\n",
        "# Function to interact with the chatbot\n",
        "def chatbot(question):\n",
        "    response = qa_chain.invoke(question)\n",
        "    return response[\"result\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVQTPmNbBYxm"
      },
      "outputs": [],
      "source": [
        "# Chatbot accuracy\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "\n",
        "# Load a sentence transformer model for semantic similarity\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Test cases with ground-truth answers\n",
        "test_set = [\n",
        "    {\"query\": \"What is the blended rate for Boeing B756 Captain Pay Rates, Aircraft 764, in year 1?\", \"expected_answer\": \"486.69\"},\n",
        "    {\"query\": \"My arrival time at FLL was delayed 4 hours. I was supposed to have a 15 hour layover and now it's 11 hours. What hotel should they provide?\", \"expected_answer\": \"a pilot scheduled for a layover should be furnished suitable single occupancy lodging in accordance with Section 4-C and the Hotel Guidelines. The hotel should be within 15 minutes normal driving time from the airport.\"}\n",
        "]\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_chatbot_accuracy(test_data, chatbot_function, threshold=0.85):\n",
        "    correct_count = 0\n",
        "    total = len(test_data)\n",
        "    detailed_results = []\n",
        "\n",
        "    for item in test_data:\n",
        "        question = item[\"query\"]\n",
        "        expected = item[\"expected_answer\"]\n",
        "        predicted = chatbot_function(question)\n",
        "\n",
        "        # Semantic similarity using SBERT\n",
        "        expected_emb = sbert_model.encode(expected, convert_to_tensor=True)\n",
        "        predicted_emb = sbert_model.encode(predicted, convert_to_tensor=True)\n",
        "        similarity_score = util.cos_sim(expected_emb, predicted_emb).item()\n",
        "\n",
        "        is_correct = similarity_score >= threshold\n",
        "        if is_correct:\n",
        "            correct_count += 1\n",
        "\n",
        "        detailed_results.append({\n",
        "            \"question\": question,\n",
        "            \"expected\": expected,\n",
        "            \"predicted\": predicted,\n",
        "            \"similarity\": round(similarity_score, 3),\n",
        "            \"is_correct\": is_correct\n",
        "        })\n",
        "\n",
        "    accuracy_percent = (correct_count / total) * 100\n",
        "    return round(accuracy_percent, 2), detailed_results\n",
        "\n",
        "# Run the evaluation\n",
        "accuracy, results = evaluate_chatbot_accuracy(test_set, chatbot)\n",
        "print(f\"\\n✅ Chatbot Accuracy: {accuracy}%\\n\")\n",
        "\n",
        "# Optional: Print detailed breakdown\n",
        "for r in results:\n",
        "    print(f\"Q: {r['question']}\\n✓ Expected: {r['expected']}\\n✎ Predicted: {r['predicted']}\\n→ Similarity: {r['similarity']} → {'✔️' if r['is_correct'] else '❌'}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnUd2WsVug1k"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import random\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Define greeting inputs and responses\n",
        "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\")\n",
        "GREETING_RESPONSES = [\"hi\", \"hey\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
        "\n",
        "def greeting(sentence):\n",
        "    \"\"\"Check if the user input contains a greeting.\"\"\"\n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            return random.choice(GREETING_RESPONSES)\n",
        "    return None\n",
        "\n",
        "'''def response(user_input):\n",
        "    \"\"\"Generate a response based on user input.\"\"\"\n",
        "    answer = chatbot(user_input)\n",
        "    return(answer)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrMgiCSgPJIW"
      },
      "outputs": [],
      "source": [
        "#Start the Chatbot\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML\n",
        "import random\n",
        "import warnings\n",
        "import sys\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "def on_submit(_):\n",
        "    \"\"\"Handle user input and generate chatbot response.\"\"\"\n",
        "    query = input_box.value.strip()\n",
        "    input_box.value = \"\"\n",
        "\n",
        "    if query.lower() in ['bye', 'exit', 'stop']:\n",
        "        display(HTML(\"<b><font color='red' style='font-size: 18px;'>Pilot_buddy:</font></b> <span style='font-size: 18px;'>Thank you for using Pilot_buddy. Have a great day!</span>\"))\n",
        "        return\n",
        "\n",
        "    greeting_response = greeting(query)\n",
        "    if greeting_response:\n",
        "        answer = greeting_response\n",
        "    else:\n",
        "        answer = chatbot(query)\n",
        "\n",
        "    display(HTML(f\"<b style='font-size: 18px;'>User:</b> <span style='font-size: 18px;'>{query}</span>\"))\n",
        "    display(HTML(f\"<b><font color='blue' style='font-size: 18px;'>Pilot_buddy:</font></b> <span style='font-size: 18px;'>{answer}</span>\"))\n",
        "\n",
        "# Display Welcome Message with Larger Font\n",
        "display(HTML(\"<h3 style='font-size: 22px;'>🚀 Welcome to Pilot_buddy! Your AI assistant for CBAs. Type 'exit' to stop.</h3>\"))\n",
        "\n",
        "# Input Box for User Queries\n",
        "input_box = widgets.Text(placeholder=\"Please enter your question...\")\n",
        "input_box.on_submit(on_submit)\n",
        "\n",
        "display(input_box)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvrgq5ugDv4f"
      },
      "source": [
        "### GUI of Chabot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6ShDBJPDzd8"
      },
      "outputs": [],
      "source": [
        "! pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yUiwutND6_8"
      },
      "outputs": [],
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhtpimAtD7PA"
      },
      "outputs": [],
      "source": [
        "! streamlit run app.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}